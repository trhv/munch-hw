# Website Summary Scraper

This project is a backend service that receives a URL and returns a structured summary of the websiteâ€™s content by scraping it. The service determines the type of page (article, product, documentation) and uses a strategy pattern to extract relevant data accordingly.

---

## ğŸš€ Features

- ğŸ” Automatically detects page type (`article`, `product`, `documentation`)
- ğŸ§  Uses Strategy Pattern to handle different page structures
- ğŸ“Š Extracts meaningful content like title, description, price, author, and more
- ğŸ“¦ Modular TypeScript codebase with utility-based architecture
- ğŸ“„ Built-in error handling and logging with a custom logger

---

## ğŸ›  Installation

1. Install dependencies:

```bash
cd website-summary-scraper
npm install
```

2. Create a .env file
```bash
cp .env.example .env
```

3. Start the development server
```bash
npm run dev
```

Server will run at
```bash
http://localhost:3000
```
## ğŸ“¬ API Usage

Endpoint
```bash
POST /scrape
```

Request Body
```bash
{
  "url": "https://example.com"
}
```

Example using curl
```bash
curl -X POST http://localhost:3000/scrape \
  -H "Content-Type: application/json" \
  -d '{ "url": "https://example.com" }'
```

Example Response (Article Page)
```bash
{
  "pageType": "article",
  "title": "Example Article",
  "metaDescription": "A short summary of the article.",
  "headings": ["Introduction", "Summary"],
  "paragraphs": ["This is the first paragraph...", "Another detail here..."],
  "author": "John Doe",
  "publishedAt": "2024-03-01"
}
```

## ğŸ§ª Running Tests

Run unit tests:
```bash
npm test
```
